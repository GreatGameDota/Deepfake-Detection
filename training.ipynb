{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport glob\nimport random\nfrom tqdm.notebook import tqdm\nimport cv2\nimport sklearn.metrics\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = glob.glob('../input/deepfake-detection-faces-*/*.csv')\nmeta.sort(key=lambda f: int(re.sub('\\D', '', f)))\n\ndfs = []\nfor path in meta:\n    df = pd.read_csv(path)\n    df['path'] = ''\n    path = path.split(\"/\")[:-1]\n    path = path[0] + '/' + path[1] + '/' + path[2] + '/'\n    for i in range(len(df)):\n        df.loc[i]['path'] = f'{path}{df.loc[i][\"filename\"][:-4]}'\n    dfs.append(df)\n\ntrain_df = pd.concat(dfs)\ntrain_df = train_df.reset_index(drop=True)\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part = 16\nfor j in range((39-16)+1):\n    if part+j != 17:\n        meta = pd.read_csv(f'../input/dfdc-part-{part+j}/images/metadata{part+j}.csv')\n    else:\n        meta = pd.read_csv(f'../input/dfdc-part-{part+j}/images/metadata{part+j}.json', index_col=0)\n    meta['path'] = ''\n    print(part+j)\n    del_idxs = []\n    for i in range(len(meta)):\n        if os.path.isdir(f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}'):\n            if len(os.listdir(f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}')) < 10:\n                del_idxs.append(i)\n            else:\n                meta.loc[i]['path'] = f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}'\n        else:\n            del_idxs.append(i)\n    print(del_idxs)\n    for idx in del_idxs:\n        meta = meta.drop(idx)\n    train_df = pd.concat([train_df,meta])\n    train_df = train_df.reset_index(drop=True)\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\npart = 40\nfor j in range(10):\n    if part+j != 17:\n        meta = pd.read_csv(f'../input/dfdc-part-{part+j}/images/metadata{part+j}.csv')\n    else:\n        meta = pd.read_csv(f'../input/dfdc-part-{part+j}/images/metadata{part+j}.json', index_col=0)\n    meta['path'] = ''\n    print(part+j)\n    del_idxs = []\n    for i in range(len(meta)):\n        if os.path.isdir(f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}'):\n            if len(os.listdir(f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}')) < 10:\n                del_idxs.append(i)\n            else:\n                meta.loc[i]['path'] = f'../input/dfdc-part-{part+j}/images/{meta.loc[i][\"filename\"][:-4]}'\n        else:\n            del_idxs.append(i)\n    print(del_idxs)\n    for idx in del_idxs:\n        meta = meta.drop(idx)\n    dfs.append(meta)\nval_df = pd.concat(dfs)\nval_df = val_df.reset_index(drop=True)\nprint(len(val_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_df = train_df\nte_df = val_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_f = tr_df.loc[tr_df['label']=='FAKE']\ntrain_r = tr_df.loc[tr_df['label']=='REAL']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nSingle image"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\nclass ImageDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        video = self.df.iloc[idx]\n        \n        imgs = glob.glob(f'{video[\"path\"]}/*')\n        if len(imgs) < 1:\n            print(video[\"path\"])\n        \n        bad = []\n        for im in imgs:\n            if len(im.split('_')) > 1:\n               bad.append(im)\n        for im in bad:\n            imgs.remove(im)\n        \n        img_path = random.sample(imgs, 1)[0]\n        img = cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (224, 224))\n\n        if self.transform is not None:\n          res = self.transform(image=img)\n          img = res['image']\n        \n        img = np.rollaxis(img, -1, 0)\n        \n        label = video['label']\n        labels = 1\n        if label == 'FAKE':\n            labels = 1\n        else:\n            labels = 0\n        labels = np.array(labels).astype(np.float32)\n        return [img, labels]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorchcv --quiet\nfrom pytorchcv.model_provider import get_model\n# model = get_model(\"seresnext50_32x4d\", pretrained=True)\n# model = get_model(\"xception\", pretrained=True)\n# model = get_model(\"inceptionv3\", pretrained=True)\n# model = get_model(\"inceptionresnetv2\", pretrained=True)\n# model = get_model(\"mobilenet_w1\", pretrained=True)\nmodel = get_model(\"efficientnet_b1\", pretrained=True)\nmodel = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n\n# model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d(1)) # xcep\nmodel[0].final_pool = nn.Sequential(nn.AdaptiveAvgPool2d(1)) #effi, incep\n\nclass Head(torch.nn.Module):\n  def __init__(self, in_f, out_f):\n    super(Head, self).__init__()\n    \n    self.f = nn.Flatten()\n    self.l = nn.Linear(in_f, 512)\n    self.d = nn.Dropout(0.75)\n    self.o = nn.Linear(512, out_f)\n    self.b1 = nn.BatchNorm1d(in_f)\n    self.b2 = nn.BatchNorm1d(512)\n    self.r = nn.ReLU()\n\n  def forward(self, x):\n    x = self.f(x)\n    x = self.d(x)\n\n    x = self.l(x)\n    x = self.r(x)\n    x = self.d(x)\n\n    out = self.o(x)\n    return out\n\nclass FCN(torch.nn.Module):\n  def __init__(self, base, in_f):\n    super(FCN, self).__init__()\n    self.base = base\n    self.h1 = Head(in_f, 1)\n  \n  def forward(self, x):\n    x = self.base(x)\n    return self.h1(x)\n\nmodel = FCN(model, 1280) # effi\n# model = FCN(model, 1536) # incep-res\n# model = FCN(model, 2048) # xcep","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LRCN Dataset and Model (unused)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from torch.utils.data import Dataset, DataLoader\n# mean = [0.485, 0.456, 0.406]\n# std = [0.229, 0.224, 0.225]\n\n# class ImageDataset(Dataset):\n#     def __init__(self, df, transform=None):\n#         self.df = df\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.df)\n    \n#     def __getitem__(self, idx):\n#         video = self.df.iloc[idx]\n        \n#         imgs = glob.glob(f'{video[\"path\"]}/*')\n#         if len(imgs) < 1:\n#             print(video[\"path\"])\n#         imgs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n        \n#         bad = []\n#         for im in imgs:\n#             if len(im.split('_')) > 1:\n#                bad.append(im)\n#         for im in bad:\n#             imgs.remove(im)\n        \n#         img_paths = imgs[:10]\n    \n#         faces = []\n#         for img_path in img_paths:\n#             img = cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB)\n#             img = cv2.resize(img, (150, 150))\n\n#             if self.transform is not None:\n#               res = self.transform(image=img)\n#               img = res['image']\n        \n#             img = np.rollaxis(img, -1, 0)\n            \n#             faces.append(img)\n#         faces = np.array(faces)\n        \n#         label = video['label']\n#         labels = 1\n#         if label == 'FAKE':\n#             labels = 1\n#         else:\n#             labels = 0\n#         labels = np.array([labels]).astype(np.float32)\n#         return [faces, labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install pytorchcv --quiet\n# from pytorchcv.model_provider import get_model\n# # model = get_model(\"xception\", pretrained=True)\n# model = get_model(\"efficientnet_b1\", pretrained=True)\n# model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n# model[0].final_pool = nn.AdaptiveAvgPool2d(1)\n# # model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d(1))\n\n# class LRCN(nn.Module):\n#     def __init__(self, base, in_f, out_f):\n#         super(LRCN, self).__init__()\n#         self.cnn = base\n        \n#         self.LSTM = nn.LSTM(\n#             input_size=in_f,\n#             hidden_size=256,\n#             num_layers=1,\n#             batch_first=True\n#         )\n\n#         self.f1 = nn.Linear(256, 128)\n#         self.f2 = nn.Linear(128, out_f)\n#         self.r = nn.ReLU()\n#         self.d = nn.Dropout(0.5)\n        \n#     def forward(self, x):\n#         batch_size, timesteps, C, H, W = x.size()\n#         x = x.view(batch_size * timesteps, C, H, W)\n#         x = self.cnn(x)\n#         x = x.view(batch_size, timesteps, -1)\n#         self.LSTM.flatten_parameters()\n#         x, (hn,hc) = self.LSTM(x)\n#         x = self.d(self.r(self.f1(x[:,-1,:])))\n#         x = self.f2(x)\n#         return x\n\n# model = LRCN(model, 1280, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def criterion1(pred1, targets):\n  l1 = F.binary_cross_entropy(F.sigmoid(pred1), targets)\n  return l1\n\ndef train_model(epoch, optimizer, scheduler=None, history=None):\n    model.train()\n    total_loss = 0\n    \n    t = tqdm(train_loader)\n    for i, (img_batch, y_batch) in enumerate(t):\n        img_batch = img_batch.cuda().float()\n        y_batch = y_batch.cuda().float()\n\n        optimizer.zero_grad()\n        \n        rand = np.random.rand()\n        if rand < 0.4:\n            images, targets = cutmix(img_batch, y_batch, 0.4)\n            output1 = model(images)\n            loss = cutmix_criterion(output1, targets)\n            loss = loss[0]\n        elif rand < 0.8:\n            images, targets = mixup(img_batch, y_batch, 0.4)\n            output1 = model(images)\n            loss = mixup_criterion(output1, targets)\n            loss = loss[0]\n        else:\n            out = model(img_batch)\n            loss = criterion1(out, y_batch)\n\n        total_loss += loss\n        t.set_description(f'Epoch {epoch+1}/{n_epochs}, LR: %6f, Loss: %.4f'%(optimizer.state_dict()['param_groups'][0]['lr'],total_loss/(i+1)))\n\n        if history is not None:\n          history.loc[epoch + i / len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n          history.loc[epoch + i / len(train_loader), 'lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n          scheduler.step()\n\ndef evaluate_model(epoch, scheduler=None, history=None):\n    model.eval()\n    loss = 0\n    pred = []\n    real = []\n    with torch.no_grad():\n        for img_batch, y_batch in tqdm(val_loader):\n            img_batch = img_batch.cuda().float()\n            y_batch = y_batch.cuda().float()\n\n            o1 = model(img_batch)\n            l1 = criterion1(o1, y_batch)\n            loss += l1\n            \n            for j in o1:\n              pred.append(F.sigmoid(j))\n            for i in y_batch:\n              real.append(i.data.cpu())\n    \n    pred = [p.data.cpu().numpy() for p in pred]\n    pred2 = pred\n    pred = [np.round(p) for p in pred]\n    pred = np.array(pred)\n    acc = sklearn.metrics.recall_score(real, pred, average='macro')\n\n    real = [r.item() for r in real]\n    pred2 = np.array(pred2).clip(0.01, 0.99)\n    kaggle = sklearn.metrics.log_loss(real, pred2)\n\n    loss /= len(val_loader)\n    \n    if history is not None:\n        history.loc[epoch, 'dev_loss'] = loss.cpu().numpy()\n    \n    if scheduler is not None:\n      scheduler.step(loss)\n\n    print(f'Dev loss: %.4f, Acc: %.6f, Kaggle: %.6f'%(loss,acc,kaggle))\n    \n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mixup/Cutmix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ohem_loss( rate, cls_pred, cls_target ):\n    batch_size = cls_pred.size(0)\n    ohem_cls_loss = F.binary_cross_entropy(F.sigmoid(cls_pred), cls_target)\n    return ohem_cls_loss\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix(data, targets1, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets1 = targets1[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n\n    targets = [targets1, shuffled_targets1, lam]\n    return data, targets\n\n# loss \ndef cutmix_criterion(preds1, targets, rate=1.0):\n    targets1, targets2, lam = targets[0], targets[1], targets[2]\n    criterion = ohem_loss\n    return [ lam * criterion(rate, preds1, targets1) + (1 - lam) * criterion(rate, preds1, targets2) ]\n\n\ndef mixup(data, targets1, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets1 = targets1[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    data = data * lam + shuffled_data * (1 - lam)\n    targets = [targets1, shuffled_targets1, lam]\n\n    return data, targets\n\n\ndef mixup_criterion(preds1, targets, rate=1.0):\n    targets1, targets2, lam = targets[0], targets[1], targets[2]\n    criterion = ohem_loss\n    return [ lam * criterion(rate, preds1, targets1) + (1 - lam) * criterion(rate, preds1, targets2) ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations as A\n\ntrain_transform = A.Compose([\n    # A.ShiftScaleRotate(p=0.3, scale_limit=0.25, border_mode=1, rotate_limit=10),\n    A.HorizontalFlip(p=0.2),\n#     A.RandomBrightnessContrast(p=0.3, brightness_limit=0.5, contrast_limit=0.5),\n#     A.OneOf([\n#       A.JpegCompression(quality_lower=8, quality_upper=30, p=1.0),\n#       A.Downscale(scale_min=0.5, scale_max=0.75, p=1.0)\n#     ], p=0.2),\n#     A.RandomCrop(110, 110, p=0.2),\n#     A.Normalize(always_apply=True)\n])\n\nval_transform = A.Compose([\n    A.Normalize(always_apply=True)\n])\nval_dataset = ImageDataset(te_df, transform=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_batch = train_f.sample(len(train_r)).reset_index(drop=True)\ntrain_ = pd.concat([fake_batch, train_r])\ntrain_ = train_.sample(frac=1).reset_index(drop=True)\nprint(train_['label'].value_counts())\n\ntrain_dataset = ImageDataset(train_, transform=train_transform)\n\nnrow, ncol = 3, 5\nfig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    images, label = train_dataset[i]\n    image = np.rollaxis(images, 0, 3)\n    ax.imshow(image)\n    ax.set_title(f'label: {label}')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\nhistory = pd.DataFrame()\nhistory2 = pd.DataFrame()\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nbest = 1e10\nn_epochs = 20\n\nbatch_size = 64\nval_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\nmodel = model.cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, mode='min', factor=0.7, verbose=True, min_lr=1e-5)\n\nfor epoch in range(n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    fake_batch = train_f.sample(len(train_r)).reset_index(drop=True)\n    train_ = pd.concat([fake_batch, train_r])\n    train_ = train_.sample(frac=1).reset_index(drop=True)\n    \n    train_dataset = ImageDataset(train_, transform=train_transform)\n    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    train_model(epoch, optimizer, scheduler=scheduler, history=history)\n    \n    loss = evaluate_model(epoch, scheduler=None, history=history2)\n    \n    if loss < best:\n      best = loss\n      print(f'Saving best model...')\n      torch.save(model.state_dict(), f'model_{epoch+1}.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2.plot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}